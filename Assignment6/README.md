# Assignment 6 &mdash; Predictive and Prescriptive Analytics for Bank Marketing

## Dataset access and reproducibility
- **Data source.** UCI Machine Learning Repository &mdash; Bank Marketing, "bank-additional" full version. Place `bank-additional-full.csv` inside `Assignment6/data/` before executing the workflow; the R script attempts to download the file automatically and will prompt for a manual copy if the network is unavailable.【F:Assignment6/Assignment6.R†L23-L56】
- **Analysis driver.** Run `Rscript Assignment6/Assignment6.R` after installing the R dependencies listed at the top of the script. The workflow saves derived tables to `Assignment6/results/` and graphics to `Assignment6/figures/` so they can be embedded in this write-up without mixing code and exposition.【F:Assignment6/Assignment6.R†L1-L218】

## 1. Exploratory data analysis
The marketing campaign contains 41,188 contact records with a markedly imbalanced outcome: only 11.3% of clients subscribed to the term deposit, highlighting the need for metrics that go beyond raw accuracy.【F:Assignment6/results/y_distribution.csv†L1-L3】【F:Assignment6/figures/class_balance.svg†L1-L12】 The majority class ("no") exceeds the positive class by roughly eight to one, making sensitivity- and precision-oriented metrics essential for fair assessment.

Numeric drivers exhibit mixed behavior. Age clusters tightly around a mean of 40 years (IQR 32&ndash;47) with a long upper tail, while call duration is heavily right-skewed (median 180 seconds, maximum 4,918 seconds), confirming that campaign engagement varies dramatically by customer.【F:Assignment6/results/numeric_summary.csv†L2-L3】 Banking context indicators such as `euribor3m` and `nr.employed` show relatively low variance because they were recorded during macroeconomic downturns; in contrast, `pdays` concentrates at 999 (no prior contact) and contributes little direct signal without further encoding.【F:Assignment6/results/numeric_summary.csv†L4-L10】 Stylized distribution sketches summarize the pronounced skew and multi-modality seen across the key drivers.【F:Assignment6/figures/numeric_distributions.svg†L1-L12】

Two derived insights guided feature engineering. First, the macroeconomic variables (`emp.var.rate`, `cons.price.idx`, `cons.conf.idx`, `euribor3m`, `nr.employed`) move together, hinting that dimension reduction might offer a compact summary without sacrificing information. Second, campaign history fields (`pdays`, `previous`, `poutcome`) differentiate a small but high-yield segment, motivating algorithms that capture rare yet predictive patterns.

## 2. Model development, validation, and optimization
Four supervised pipelines and one unsupervised study were built to satisfy the graduate-level requirement and cover both predictive and descriptive objectives:

1. **Baseline logistic regression.** A generalized linear model with dummy-encoded categorical features and standardized numeric inputs served as the benchmark.【F:Assignment6/Assignment6.R†L85-L134】 It produced an ROC AUC of 0.939, PR AUC of 0.492, and 91.0% accuracy, but sensitivity remained modest (56.3%) because of the class imbalance.【F:Assignment6/results/model_test_metrics.csv†L2-L6】 The confusion matrix reveals that most errors are missed subscriptions (false negatives), suggesting threshold adjustment or class weights as follow-up options.【F:Assignment6/results/logistic_confusion_matrix.csv†L1-L5】
2. **PCA-augmented logistic regression.** Introducing a PCA step retaining 90% variance mainly compresses correlated macroeconomic drivers. This sacrifice of granular information decreased ROC AUC to 0.925 and sensitivity to 53.4%, confirming that direct macro variables hold useful nuance even after normalization.【F:Assignment6/Assignment6.R†L113-L135】【F:Assignment6/results/model_test_metrics.csv†L7-L11】【F:Assignment6/results/pca_logistic_confusion_matrix.csv†L1-L5】
3. **Random forest.** A tuned ranger implementation (`trees = 1000`, `mtry`/`min_n` optimized via 5-fold cross-validation) captured higher-order interactions, raising sensitivity to 59.7% while maintaining 96.1% specificity and 0.951 ROC AUC.【F:Assignment6/Assignment6.R†L136-L205】【F:Assignment6/results/model_test_metrics.csv†L12-L16】【F:Assignment6/results/random_forest_confusion_matrix.csv†L1-L5】 Feature importances emphasized call duration, euro-area rates, employment levels, and successful outcomes from prior campaigns, aligning with domain expectations.【F:Assignment6/figures/xgb_variable_importance.svg†L1-L20】
4. **Gradient boosted trees (XGBoost).** Fine-tuned learning rate, depth, and leaf size delivered the strongest overall discrimination: ROC AUC 0.958, PR AUC 0.562, 92.2% accuracy, and the highest sensitivity (64.1%).【F:Assignment6/Assignment6.R†L136-L205】【F:Assignment6/results/model_test_metrics.csv†L17-L21】【F:Assignment6/results/xgboost_confusion_matrix.csv†L1-L5】 The boosted model improves recall without severely sacrificing specificity, making it the preferred production candidate. Variable influence mirrors the forest but with sharper emphasis on recent campaign performance and macro conditions.【F:Assignment6/figures/xgb_variable_importance.svg†L1-L20】
5. **K-means clustering on numeric drivers.** A 2-cluster solution maximized silhouette score (0.246) and partitioned the training population into a large, younger, low-duration group versus a smaller, older, long-duration group with better historic engagement.【F:Assignment6/Assignment6.R†L207-L218】【F:Assignment6/results/silhouette_scores.csv†L1-L6】【F:Assignment6/results/cluster_summary.csv†L1-L3】【F:Assignment6/figures/kmeans_clusters.svg†L1-L14】 The segmentation supports differentiated marketing tactics even when subscription labels are withheld.

Cross-validation folds were stratified to preserve the rare positive class, and hyperparameter grids were constrained to realistic search spaces to balance computational efficiency with expressiveness.【F:Assignment6/Assignment6.R†L78-L112】【F:Assignment6/Assignment6.R†L136-L178】 The pipeline architecture keeps preprocessing consistent across models by reusing recipes where appropriate, ensuring fair comparisons and reproducibility.

## 3. Prescriptive insights and decisions
The ensemble of models underscores several actionable points:

- **Prioritize duration and macroeconomic context.** Both tree ensembles and logistic regression highlight call duration, euribor rates, and employment indices as dominant predictors, suggesting that aligning outreach with favorable macro conditions and investing in longer, more informative calls pays dividends.【F:Assignment6/results/model_test_metrics.csv†L12-L21】【F:Assignment6/figures/xgb_variable_importance.svg†L1-L20】
- **Segment outreach strategies.** The clustering analysis shows that older clients with longer engagement histories form a compact, high-yield niche. Tailored messaging and possibly proactive callbacks for this segment can raise conversion without broadly increasing campaign pressure.【F:Assignment6/results/cluster_summary.csv†L1-L3】【F:Assignment6/figures/kmeans_clusters.svg†L1-L14】
- **Adopt cost-sensitive thresholds.** Even the boosted model leaves one third of potential subscribers unmatched; applying business-driven thresholds (e.g., targeting a 0.30 predicted probability) or ensembling with uplift modeling can trade a small increase in follow-up calls for materially higher recall.【F:Assignment6/results/xgboost_confusion_matrix.csv†L1-L5】
- **Monitor data drift.** Because macroeconomic inputs dominate the signal, routine retraining is essential when market conditions shift. The PCA experiment demonstrates that aggressive dimensionality reduction can mask subtle drifts, so preserving raw macro indicators in monitoring dashboards is advisable.【F:Assignment6/results/model_test_metrics.csv†L7-L11】

Overall, XGBoost offers the best balance between discrimination and operational cost, while the unsupervised clusters inform differentiated campaign tactics. Combining the boosted model with a cluster-aware outreach plan promises improved subscription rates with manageable resource allocation.
