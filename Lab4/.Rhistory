# Build a numeric+dummied matrix for clustering (train+test combined)
# Use caret's dummyVars to one-hot encode
dv <- dummyVars(~ . - y, data = dat, fullRank = TRUE)
X_all <- predict(dv, dat)
# Scale numeric columns
X_all <- scale(X_all)
k_vals <- 2:8
sil_scores <- numeric(length(k_vals))
for (i in seq_along(k_vals)) {
set.seed(42)
km <- kmeans(X_all, centers = k_vals[i], nstart = 20)
sil <- silhouette(km$cluster, dist(X_all))
sil_scores[i] <- mean(sil[, 3])
}
best_k <- sil_df$k[which.max(sil_df$mean_silhouette)]
cat("Best k by silhouette:", best_k, "\n")
set.seed(42)
km_best <- kmeans(X_all, centers = best_k, nstart = 25)
# Simple numeric profile by cluster for first few numeric columns
profile_cols <- intersect(c("age","balance","duration","campaign","pdays","previous"), names(dat))
if (length(profile_cols) == 0) profile_cols <- num_cols[1:min(4, length(num_cols))]
prof <- aggregate(dat[, profile_cols, drop = FALSE], by = list(cluster = clusters), FUN = function(x) mean(as.numeric(x), na.rm = TRUE))
print(prof)
## 11) Threshold analysis → prescriptive decision -----------------------------
thr_seq <- seq(0.01, 0.99, by = 0.01)
f1_at_thr <- sapply(thr_seq, function(t) {
pred_class <- factor(ifelse(best_probs >= t, "yes", "no"), levels = c("yes","no"))
# Confusion-based F1 (positive = "yes")
caret::F_meas(pred_class, test$y, relevant = "yes")
})
thr_df <- data.frame(threshold = thr_seq, F1 = f1_at_thr)
print(ggplot(thr_df, aes(threshold, F1)) + geom_line() + labs(title = paste("F1 vs Threshold —", best_name)))
best_thr <- thr_df$threshold[which.max(thr_df$F1)]
cat(sprintf("Recommended operating threshold (max F1): %.3f\n", best_thr))
decision_table <- data.frame(
Band = c("High","Medium","Low"),
ScoreRange = c(sprintf("[%.2f – 1.00]", best_thr),
sprintf("[%.2f – %.2f)", 0.6*best_thr, best_thr),
sprintf("[0.00 – %.2f)", 0.6*best_thr)),
RecommendedAction = c("Target immediately (email + call)",
"Target by email only; A/B test message",
"Hold; re-score next campaign")
)
print(decision_table)
cat("\nDONE. Include in your report: class-balance, a couple hist/boxplots, metrics table,\n",
"confusion matrix, ROC & PR curves, top-20 var importance, silhouette-by-k plot,\n",
"and the F1-vs-threshold plot + the decision table.\n")
### DataAnalytics2025Fall_A6_Elizabeth_Rice.R
### Bank Marketing (UCI) — 6000-level: 4+ models + PCA + Clustering
### Self-contained caret-based pipeline (no tidymodels required)
## 0) Install & load packages -----------------------------------------------
needed <- c("caret","glmnet","randomForest","ranger","e1071","pROC","precrec","cluster","ggplot2")
to_get <- needed[!(needed %in% rownames(installed.packages()))]
if(length(to_get)) install.packages(to_get, dependencies = TRUE)
suppressPackageStartupMessages({
library(caret)       # modeling framework
library(glmnet)      # penalized logistic
library(randomForest)# RF (ranger also available)
library(ranger)
library(e1071)       # SVM radial (via caret)
library(pROC)        # ROC
library(precrec)     # PR curves
library(cluster)     # silhouette
library(ggplot2)
})
set.seed(42)
theme_set(theme_minimal())
## 1) Locate and load data ---------------------------------------------------
DATA_DIR <- "/Users/elizabethrice/Desktop/Data Analytics Labs/Assignment6/"
# Prefer bank-additional-full.csv; else bank-full.csv
cands <- c("bank-additional-full.csv", "bank-full.csv")
file_path <- NULL
for (fn in cands) {
p <- file.path(DATA_DIR, fn)
if (file.exists(p)) { file_path <- p; break }
}
if (is.null(file_path)) stop("Couldn't find bank-additional-full.csv or bank-full.csv in ", DATA_DIR)
# These UCI files are semicolon-delimited with header
dat <- read.csv(file_path, sep = ";", stringsAsFactors = TRUE)
# Normalize column names
names(dat) <- make.names(names(dat))
# Target variable: y (yes/no) ⇒ caret expects positive class first by default; we’ll set levels explicitly
if (!"y" %in% names(dat)) stop("Target column 'y' not found.")
dat$y <- factor(tolower(as.character(dat$y)), levels = c("yes","no"))  # positive level first for ROC
# If NA introduced due to other strings, coerce appropriately:
dat$y[is.na(dat$y)] <- "no"
dat$y <- factor(dat$y, levels=c("yes","no"))
cat("Loaded:", basename(file_path), " | Rows:", nrow(dat), " Cols:", ncol(dat), "\n")
## 2) Quick EDA --------------------------------------------------------------
# Class balance
print(ggplot(dat, aes(x = y)) + geom_bar() + labs(title = "Class Balance (y)", x = "", y = "Count"))
# Identify numeric columns
num_cols <- names(dat)[sapply(dat, is.numeric)]
# Show histograms for up to first 4 numeric columns
for (c in head(num_cols, 4)) {
print(ggplot(dat, aes(x = .data[[c]])) +
geom_histogram(bins = 30) +
labs(title = paste("Histogram:", c), x = c, y = "Count"))
}
# Optional: a trimmed boxplot if 'balance' exists in this variant
if ("balance" %in% names(dat)) {
q <- quantile(dat$balance, c(.01, .99), na.rm = TRUE)
print(ggplot(dat, aes(x = y, y = balance, fill = y)) +
geom_boxplot(show.legend = FALSE) +
coord_cartesian(ylim = q) +
labs(title = "Balance by Outcome (trimmed)", x = "", y = "Balance"))
}
## 3) Train / Test split -----------------------------------------------------
set.seed(42)
train_idx <- createDataPartition(dat$y, p = 0.80, list = FALSE)
train <- dat[train_idx, ]
test  <- dat[-train_idx, ]
## 4) Modeling design matrices (dummy variables) -----------------------------
# caret can handle dummy vars via preProcess or formula + model.matrix
# We'll let caret handle it: use formula y ~ . and allow it to dummy internally.
ctrl <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,   # optimizes ROC by default
savePredictions = "final"
)
## 5) Models (≥4) ------------------------------------------------------------
# 5.1 Logistic (glmnet) — with center/scale
set.seed(42)
fit_glmnet <- train(
y ~ ., data = train,
method = "glmnet",
metric = "ROC",
trControl = ctrl,
preProcess = c("center","scale"),
tuneLength = 15
)
### DataAnalytics2025Fall_A6_Elizabeth_Rice.R
### Bank Marketing (UCI) — 6000-level: 4+ models + PCA + Clustering
### Self-contained caret-based pipeline (no tidymodels required)
## 0) Install & load packages -----------------------------------------------
needed <- c("caret","glmnet","randomForest","ranger","e1071","pROC","precrec","cluster","ggplot2")
to_get <- needed[!(needed %in% rownames(installed.packages()))]
if(length(to_get)) install.packages(to_get, dependencies = TRUE)
suppressPackageStartupMessages({
library(caret)       # modeling framework
library(glmnet)      # penalized logistic
library(randomForest)# RF (ranger also available)
library(ranger)
library(e1071)       # SVM radial (via caret)
library(pROC)        # ROC
library(precrec)     # PR curves
library(cluster)     # silhouette
library(ggplot2)
})
set.seed(42)
theme_set(theme_minimal())
## 1) Locate and load data ---------------------------------------------------
DATA_DIR <- "/Users/elizabethrice/Desktop/Data Analytics Labs/Assignment6/"
# Prefer bank-additional-full.csv; else bank-full.csv
cands <- c("bank-additional-full.csv", "bank-full.csv")
file_path <- NULL
for (fn in cands) {
p <- file.path(DATA_DIR, fn)
if (file.exists(p)) { file_path <- p; break }
}
if (is.null(file_path)) stop("Couldn't find bank-additional-full.csv or bank-full.csv in ", DATA_DIR)
# These UCI files are semicolon-delimited with header
dat <- read.csv(file_path, sep = ";", stringsAsFactors = TRUE)
# Normalize column names
names(dat) <- make.names(names(dat))
# Target variable: y (yes/no) ⇒ caret expects positive class first by default; we’ll set levels explicitly
if (!"y" %in% names(dat)) stop("Target column 'y' not found.")
dat$y <- factor(tolower(as.character(dat$y)), levels = c("yes","no"))  # positive level first for ROC
# If NA introduced due to other strings, coerce appropriately:
dat$y[is.na(dat$y)] <- "no"
dat$y <- factor(dat$y, levels=c("yes","no"))
cat("Loaded:", basename(file_path), " | Rows:", nrow(dat), " Cols:", ncol(dat), "\n")
## 2) Quick EDA --------------------------------------------------------------
# Class balance
print(ggplot(dat, aes(x = y)) + geom_bar() + labs(title = "Class Balance (y)", x = "", y = "Count"))
# Identify numeric columns
num_cols <- names(dat)[sapply(dat, is.numeric)]
# Show histograms for up to first 4 numeric columns
for (c in head(num_cols, 4)) {
print(ggplot(dat, aes(x = .data[[c]])) +
geom_histogram(bins = 30) +
labs(title = paste("Histogram:", c), x = c, y = "Count"))
}
# Optional: a trimmed boxplot if 'balance' exists in this variant
if ("balance" %in% names(dat)) {
q <- quantile(dat$balance, c(.01, .99), na.rm = TRUE)
print(ggplot(dat, aes(x = y, y = balance, fill = y)) +
geom_boxplot(show.legend = FALSE) +
coord_cartesian(ylim = q) +
labs(title = "Balance by Outcome (trimmed)", x = "", y = "Balance"))
}
## 3) Train / Test split -----------------------------------------------------
set.seed(42)
train_idx <- createDataPartition(dat$y, p = 0.80, list = FALSE)
train <- dat[train_idx, ]
test  <- dat[-train_idx, ]
## 4) Modeling design matrices (dummy variables) -----------------------------
# caret can handle dummy vars via preProcess or formula + model.matrix
# We'll let caret handle it: use formula y ~ . and allow it to dummy internally.
ctrl <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,   # optimizes ROC by default
savePredictions = "final"
)
## 5) Models (≥4) ------------------------------------------------------------
# 5.1 Logistic (glmnet) — with center/scale
set.seed(42)
fit_glmnet <- train(
y ~ ., data = train,
method = "glmnet",
metric = "ROC",
trControl = ctrl,
preProcess = c("center","scale"),
tuneLength = 15
)
# =========================
# 0) Setup & Dependencies
# =========================
set.seed(42)
need <- c(
"readr","dplyr","ggplot2","caret","randomForest","pROC","precrec",
"rsample","yardstick","glmnet","forcats","stringr"
)
to_install <- setdiff(need, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dependencies = TRUE)
suppressPackageStartupMessages({
library(readr)
library(dplyr)
library(ggplot2)
library(caret)
library(randomForest)
library(pROC)
library(precrec)
library(rsample)
library(yardstick)
library(glmnet)
library(forcats)
library(stringr)
})
# Set your local data directory (where the CSVs live)
DATA_DIR <- "/Users/elizabethrice/Desktop/Data Analytics Labs/Assignment6/"
# Prefer the more feature-rich “bank-additional” dataset if present; otherwise fallback.
PREFERRED <- file.path(DATA_DIR, "bank-additional-full.csv")
FALLBACK  <- file.path(DATA_DIR, "bank-full.csv")
DATA_PATH <- if (file.exists(PREFERRED)) PREFERRED else FALLBACK
if (!file.exists(DATA_PATH)) stop("Could not find bank marketing CSV in: ", DATA_DIR)
# Helper: clean names (avoid extra dependency on janitor)
clean_names <- function(x) {
x <- gsub("[^A-Za-z0-9]+", "_", x)
x <- gsub("^_|_$", "", x)
tolower(x)
}
# ==================================
# 1) Load & Basic Data Understanding
# ==================================
# The CSVs are ; separated.
df_raw <- read_delim(DATA_PATH, delim = ";", show_col_types = FALSE)
names(df_raw) <- clean_names(names(df_raw))
# Standardize target name (`y`) and positive class
if (!"y" %in% names(df_raw)) stop("Target column 'y' not found.")
df_raw$y <- factor(df_raw$y, levels = c("no","yes"))  # ensure level order; positive = "yes"
# Remove leakage variable `duration` (UCI note: duration highly leaks outcome)
if ("duration" %in% names(df_raw)) {
df_raw <- select(df_raw, -duration)
}
# ====================================
# 2) Preprocessing & Data Preparation
# ====================================
# Treat 'unknown' in categorical features as NA, then impute.
is_cat <- sapply(df_raw, is.character) | sapply(df_raw, is.factor)
for (cn in names(df_raw)[is_cat]) {
df_raw[[cn]] <- as.character(df_raw[[cn]])
df_raw[[cn]][tolower(df_raw[[cn]]) %in% c("unknown","")] <- NA
df_raw[[cn]] <- factor(df_raw[[cn]])
}
# Split Train/Test (80/20) stratified on y
split_obj <- initial_split(df_raw, prop = 0.80, strata = y)
train <- training(split_obj)
test  <- testing(split_obj)
# Mode imputer for factors
mode_impute_factor <- function(f) {
if (!is.factor(f)) return(f)
tab <- table(f, useNA = "no")
if (length(tab) == 0) return(f)
m <- names(which.max(tab))
f[is.na(f)] <- m
droplevels(f)
}
# Apply imputations
train_imp <- train
for (cn in names(train_imp)) {
if (is.numeric(train_imp[[cn]])) {
# median impute numerics
med <- median(train_imp[[cn]], na.rm = TRUE)
train_imp[[cn]][is.na(train_imp[[cn]])] <- med
} else {
train_imp[[cn]] <- mode_impute_factor(train_imp[[cn]])
}
}
# Keep the same imputers for test
test_imp <- test
for (cn in names(test_imp)) {
if (is.numeric(test_imp[[cn]])) {
med <- median(train_imp[[cn]], na.rm = TRUE)
test_imp[[cn]][is.na(test_imp[[cn]])] <- med
} else {
# align levels to train
test_imp[[cn]] <- factor(test_imp[[cn]], levels = levels(train_imp[[cn]]))
test_imp[[cn]] <- mode_impute_factor(test_imp[[cn]])
}
}
# Make dummy variables for all predictors except y
dmy <- dummyVars(y ~ ., data = train_imp, fullRank = TRUE)
X_train <- as.data.frame(predict(dmy, newdata = train_imp))
X_test  <- as.data.frame(predict(dmy, newdata = test_imp))
y_train <- train_imp$y
y_test  <- test_imp$y
# Ensure column alignment
common_cols <- intersect(names(X_train), names(X_test))
X_train <- X_train[, common_cols, drop = FALSE]
X_test  <- X_test[,  common_cols, drop = FALSE]
# =================================
# 3) Exploratory Data (quick views)
# =================================
# Save a couple of quick plots (optional)
OUT_DIR <- file.path(DATA_DIR, "A6_outputs")
if (!dir.exists(OUT_DIR)) dir.create(OUT_DIR, recursive = TRUE)
# Example: class balance
ggplot(data.frame(y = y_train), aes(x = y)) +
geom_bar() +
ggtitle("Class balance (train)") +
xlab("y") + ylab("count")
ggsave(file.path(OUT_DIR, "01_class_balance_train.png"), width = 6, height = 4, dpi = 150)
# Example: job vs y (if job exists)
if ("job" %in% names(train_imp)) {
p <- train_imp %>%
count(job, y, name = "n") %>%
group_by(job) %>% mutate(pct = n / sum(n)) %>% ungroup() %>%
ggplot(aes(x = fct_reorder(job, pct, .fun = max), y = pct, fill = y)) +
geom_col(position = "stack") +
coord_flip() +
ylab("within-job fraction") + xlab("job") +
ggtitle("Response rate by job (train)")
ggsave(file.path(OUT_DIR, "02_response_by_job_train.png"), p, width = 7.5, height = 6, dpi = 150)
}
# caret setup (optimize ROC)
ctrl <- trainControl(
method = "repeatedcv",
number = 5,
repeats = 2,
classProbs = TRUE,
summaryFunction = twoClassSummary,
sampling = "up" # simple handling of class imbalance (upsample minority)
)
# ------ 4.1 Logistic Regression (GLM) ------
glm_fit <- train(
x = X_train, y = y_train,
method = "glm",
family = binomial(),
metric = "ROC",
trControl = ctrl
)
# ------ 4.2 Elastic Net (GLMNET) ------
enet_grid <- expand.grid(alpha = c(0, 0.5, 1), lambda = 10^seq(-3, 0, length = 10))
enet_fit <- train(
x = X_train, y = y_train,
method = "glmnet",
family = "binomial",
metric = "ROC",
tuneGrid = enet_grid,
trControl = ctrl
)
# ------ 4.3 Random Forest ------
rf_fit <- train(
x = X_train, y = y_train,
method = "rf",
metric = "ROC",
trControl = ctrl,
tuneGrid = expand.grid(mtry = pmax(1, floor(c(ncol(X_train)*c(.05,.1,.2))))),
ntree = 500
)
# ===========================
# 5) Validation / Evaluation
# ===========================
predict_proba <- function(fit, X) {
predict(fit, newdata = X, type = "prob")[, "yes"]
}
pp_glm  <- predict_proba(glm_fit,  X_test)
pp_enet <- predict_proba(enet_fit, X_test)
pp_rf   <- predict_proba(rf_fit,   X_test)
# AUC-ROC
auc_glm  <- auc(roc(response = y_test, predictor = pp_glm,  levels = c("no","yes"), direction = "<"))
auc_enet <- auc(roc(response = y_test, predictor = pp_enet, levels = c("no","yes"), direction = "<"))
auc_rf   <- auc(roc(response = y_test, predictor = pp_rf,   levels = c("no","yes"), direction = "<"))
cat("\n--- Holdout ROC AUC ---\n")
cat(sprintf("Logistic:     %.4f\n", as.numeric(auc_glm)))
cat(sprintf("Elastic Net:  %.4f\n", as.numeric(auc_enet)))
cat(sprintf("RandomForest: %.4f\n", as.numeric(auc_rf)))
# PR AUC
mm <- mmdata(scores = c(pp_glm, pp_enet, pp_rf),
labels = rep(as.integer(y_test == "yes"), 3),
modnames = c("GLM","ENET","RF"))
mm <- evalmod(mm)
pr_tbl <- as_tibble(auc(mm)) %>% filter(curvetypes == "PRC")
cat("\n--- Holdout PR AUC ---\n")
print(pr_tbl)
# Save ROC curves
plot_roc <- function(scores, truth, title, path) {
r <- roc(truth, scores, levels = c("no","yes"), direction = "<")
png(path, width = 700, height = 500, res = 120)
plot(r, main = title, print.auc = TRUE)
dev.off()
}
plot_roc(pp_glm,  y_test, "ROC — Logistic",     file.path(OUT_DIR, "03_roc_logistic.png"))
plot_roc(pp_enet, y_test, "ROC — Elastic Net",  file.path(OUT_DIR, "04_roc_enet.png"))
plot_roc(pp_rf,   y_test, "ROC — Random Forest",file.path(OUT_DIR, "05_roc_rf.png"))
# ==========================================
# 6) Prescriptive Analytics: Threshold & ROI
# ==========================================
# Suppose:
#   • contacting a customer costs $c = 1 unit
#   • if a contacted "yes" converts, benefit = $b = 10 units
# We choose a probability threshold τ to maximize expected profit.
c_cost <- 1
b_benefit <- 10
threshold_grid <- seq(0.05, 0.95, by = 0.01)
prescriptive_eval <- function(scores, truth, c_cost, b_benefit, grid) {
res <- lapply(grid, function(tau) {
pred <- factor(ifelse(scores >= tau, "yes", "no"), levels = c("no","yes"))
cm <- table(pred, truth)
tp <- cm["yes","yes"]; fp <- cm["yes","no"]; fn <- cm["no","yes"]; tn <- cm["no","no"]
tp <- ifelse(is.na(tp), 0, tp); fp <- ifelse(is.na(fp), 0, fp)
# Profit = benefit * TP - cost * (TP + FP)
profit <- b_benefit * tp - c_cost * (tp + fp)
data.frame(threshold = tau, tp = tp, fp = fp, fn = fn, tn = tn, profit = profit)
})
bind_rows(res)
}
pres_glm  <- prescriptive_eval(pp_glm,  y_test, c_cost, b_benefit, threshold_grid) %>% mutate(model="GLM")
pres_enet <- prescriptive_eval(pp_enet, y_test, c_cost, b_benefit, threshold_grid) %>% mutate(model="ENET")
pres_rf   <- prescriptive_eval(pp_rf,   y_test, c_cost, b_benefit, threshold_grid) %>% mutate(model="RF")
pres_all  <- bind_rows(pres_glm, pres_enet, pres_rf)
# Best threshold per model
best_by_model <- pres_all %>%
group_by(model) %>%
slice_max(order_by = profit, n = 1, with_ties = FALSE) %>%
ungroup()
cat("\n--- Profit-Optimizing Thresholds (c=1, b=10) ---\n")
print(best_by_model)
# Save profit curves
ggplot(pres_all, aes(x = threshold, y = profit, color = model)) +
geom_line() +
geom_point(data = best_by_model, size = 2) +
ggtitle("Profit vs Threshold by Model (contact cost=1, conversion benefit=10)") +
xlab("Probability threshold") + ylab("Profit (arbitrary units)")
ggsave(file.path(OUT_DIR, "06_profit_thresholds.png"), width = 7.5, height = 5.5, dpi = 150)
# Confusion matrices at best τ per model (holdout)
make_cm <- function(scores, truth, tau) {
pred <- factor(ifelse(scores >= tau, "yes", "no"), levels = c("no","yes"))
confusionMatrix(pred, truth, positive = "yes")
}
cm_glm  <- make_cm(pp_glm,  y_test, best_by_model$threshold[best_by_model$model=="GLM"])
cm_enet <- make_cm(pp_enet, y_test, best_by_model$threshold[best_by_model$model=="ENET"])
cm_rf   <- make_cm(pp_rf,   y_test, best_by_model$threshold[best_by_model$model=="RF"])
cat("\n--- Confusion Matrices at Profit-Optimal τ ---\n")
print(cm_glm)
print(cm_enet)
print(cm_rf)
# ==========================================
# 7) Final Selection & Scoring File for Action
# ==========================================
# Choose the model with highest profit at its optimal τ
best_model_name <- best_by_model$model[which.max(best_by_model$profit)]
best_tau <- best_by_model$threshold[which.max(best_by_model$profit)]
best_scores <- switch(best_model_name,
"GLM"  = pp_glm,
"ENET" = pp_enet,
"RF"   = pp_rf)
final_pred <- ifelse(best_scores >= best_tau, 1L, 0L) # 1 = contact, 0 = do not contact
action_df <- test_imp %>%
mutate(score = best_scores,
contact = final_pred) %>%
select(score, contact, everything())
write_csv(action_df, file.path(OUT_DIR, sprintf("07_action_scoring_%s.csv", tolower(best_model_name))))
cat(sprintf("\nAction file saved: %s\n", file.path(OUT_DIR, sprintf("07_action_scoring_%s.csv", tolower(best_model_name)))))
cat(sprintf("Selected model: %s at τ=%.2f\n", best_model_name, best_tau))
# ==========================================
# 8) Optional: Segmentation (Simple K-means)
# ==========================================
# Create a simple numeric feature space from dummies and run a light kmeans
km_k <- 4
set.seed(42)
km <- kmeans(scale(X_train), centers = km_k, nstart = 10)
seg_df <- train_imp %>% mutate(segment = factor(km$cluster[as.numeric(rownames(X_train))]))
seg_rate <- seg_df %>%
count(segment, y, name = "n") %>%
group_by(segment) %>% mutate(rate_yes = n / sum(n)) %>%
filter(y == "yes") %>% ungroup() %>% arrange(desc(rate_yes))
print(seg_rate)
# ==========================================
# 9) Model Objects & Session Info
# ==========================================
saveRDS(list(glm = glm_fit, enet = enet_fit, rf = rf_fit,
dummies = dmy, best_model = best_model_name, best_tau = best_tau),
file = file.path(OUT_DIR, "08_models_and_preprocessing.rds"))
writeLines(capture.output(sessionInfo()), con = file.path(OUT_DIR, "09_sessionInfo.txt"))
cat("\n=== DONE ===\n")
## ----- 0) Hard reset any open sinks (sometimes fixes "invalid connection") -----
while (sink.number() > 0) sink()
while (sink.number(type = "message") > 0) sink(type = "message")
